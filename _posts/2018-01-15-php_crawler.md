--- 
layout: post
comments: true
title: 网络爬虫
date: 2018-01-15
tag: PHP
---


网页爬虫（又被称为网页蜘蛛，网络机器人，在FOAF社区中间，更经常的称为网页追逐者），是一种按照一定规则，自动地抓取万维网信息的程序或者脚本，另外一些不常使用的名字还有蚂蚁、自动索引、模拟程序或者蠕虫。

![网络爬虫通用架构](/images/posts/common/crawler.png)


###  目录

* [产生背景](#background)
* [面临的问题](#problem)
* [分类](#category)
* [通用网络爬虫](#common-crawler)
* [聚集网络爬虫](#jujiao-crawler)
* [增量式网络爬虫](#add-crawler)
* [Deep Web 爬虫](#deep-web-crawler)


### <a name="background"></a>差生背景

随之网络的迅速发展，万维网成为大量信息的载体，如何有效地获取并利用这些信息成为一个巨大的挑战。搜索引擎（Search Engine），作为一个辅助人们检索信息的工具成为用户访问万维网的入口和指南。但是，这些通用型搜索引擎也存在着一定的局限性

* 不同领域、不同背景的用户往往具有不同的检索目的和需求，通用搜索引擎返回的结果包含大量用户不关心的内容

* 通用搜索引擎的目标去尽可能大的网络覆盖率，有限的搜索引擎资源与无限的网络数据资源之间的矛盾进一步加深

* 万维网数据形式的丰富和网络技术的不断发展，图片、数据库、音频、视频多媒体等不同数据大量出现，通用搜索引擎往往对这些信息含量密集切具有一定结构的数据无能为力，不能很好的发现和获取

* 通用搜索引擎大多提供基于关键字的检索，难以很好的发现和获取

为了解决上述问题，定向抓取相关网页资源的聚焦爬虫应运而生。聚焦爬虫是一个自动下载网页的程序，它根据既定的抓取目标，有选择的访问万维网上的网页与相关的链接，获取所需要的信息。与通用爬虫不同，聚焦爬虫并不追求大的覆盖，而将目标定位抓取某一特定主题内容相关的网页，为面向主题的用户查询准备数据资源。


####  聚焦爬虫工作原理以及关键技术概述

网络爬虫是一个自动提取网页的程序，它为搜索引擎从万维网上下载网页，是搜索亲戚的重要组成。传统爬虫从一个或若干个初始网页的URL开始，在抓取网页的过程中，不断从当前页面上抽取新的URL放入队列，知道满足系统的一定停止条件。聚焦爬虫的工作流程较为复杂，需要根据一定的网页分析算法过滤与主题无关的链接，保留有用的链接并将其放入等待抓取的URL队列。然后，它将根据一定的搜索策略从队列中选择下一步要抓取的网页URL，并重新上述过程，直到达到系统的某一条件时停止。另外，所有被爬虫抓取的网页将会被系统存储，进行一定的分析、过滤，并建立索引，以便之后的查询和检索。


###  <a name="problem"></a>面临的问题


Internet上网页的数据是庞大的，但是有很多页面和内容是重复的；动态页面的存在：客户端、服务器端脚本语言的应用使得指向相同web信息的URL数量呈指数及增长。上述特征使得网络爬虫面临一定的困难，主要体现在web信息的巨大容量使得爬虫在给定时间内只能下载少量页面。研究表明没有哪个搜索引擎能索引超出16%的internet上web页面，即使能够提取全部页面，也没有足够的空间来存储。


为来提高爬行效率，爬虫需要在单位时间内尽可能多的获取高质量页面，是它面临的难题之一。当前有五种表示页面质量高低的方式： Slimilarity(页面与爬虫主题的相似度)、 BackLink(页面在web图中的入度大小)、PageRank(指向它的所有页面平均权值之和)、ForwardLink(页面在web图中的出度大小)、Location(页面的信息位置)、Parallel(并行性问题)。 为了提高爬行速度，网络通常会采取并行爬行的工作方式，随之引入新的问题：重复性（并行运行的爬虫或爬行路线同时运行增加了重复页面）、质量问题（并行运行时，每个爬虫或爬行线程只能获取部分页面，导致页面质量下降）、通信宽带代价（并行运行时，每个爬虫或爬行线程之间不可避免的要进行一些通信）。并行运行时，网络爬虫通常采取三种方式：独立方式（各个爬虫独立爬行页面，互不通信）、动态分配释放（由一个中央协调器动态协调分配URL给各个爬虫）、静态分配方式（URL事先划分给各个爬虫）.


###  <a name="category"></a>分类


网络爬虫按照系统结构和实现技术，大致可以分为以下几种类型：通用网络爬虫、聚焦网络爬虫、增量式网络爬虫、深层网络爬虫


####  <a name="common-crawler"></a>通用网络爬虫

通用网络爬虫又称全网爬虫，爬行对象从一些种子URL扩充到整个web， 主要为门户站点所搜引擎和大型web服务提供商采集数据。由于商业原因，它们的技术细节很少公布出来。这类网络爬虫的爬行范围和数量巨大，对于爬行速度和存储空间要求较高，对于爬行页面的顺序要求相对较低，同时由于待刷新的页面太多，通常采用并行工作方式，但需要较长时间才能刷新一次页面。虽然存在一定缺陷，通用网络爬虫适用于为搜索引擎搜索广泛的主题，有较强的应用价值。


通用网络爬虫的结构大致可以分为页面爬行模块、页面分析模块、链接过滤模块、页面数据库、URL队列、初始URL集合几个部分。为提供工作效率，通用网络爬虫会采取一定的爬行策略。常用的爬行策略有：深度优先策略、广度优先策略。

* 深度优先策略： 其基本方法是按照深度由低到高的顺序，依次访问下一级网络链接，直到不能在深入为止。爬虫在完成一个爬行分支后返回到上一链接节点进一步搜索其他链接。当所有链接遍历完后，爬行任务结束。这种策略比较适合垂直搜索或站内搜索，但爬行页面内容层次较深的站点时会造成资源的巨大浪费

* 广度优先策略：此策略按照网页内容目录层次深浅来爬行页面，处于较浅目录层次的页面首先被爬行。当同一层次中的页面爬行完毕后，爬虫再深入下一层继续爬行。这种策略能够有效控制页面的爬行深度，避免遇到一个无穷深层分支时无法结束爬行的问题，实现方便，无需存储大量中间节点，不足之处在于需要较长时间才能爬行到目录层次较深的页面


#### <a name="jujiao-crawler"></a>聚焦网络爬虫 


聚焦网络爬虫，是指选择性地爬行那些与预先定义好地主题相关页面地爬虫。和通用网络爬虫相比，聚焦爬虫只需要爬行与主题相关地页面，极大地节省了硬件和网络资源，保存地页面也由于数量少而更新快，还可以很好地满足一些特定人群对特定领域信息地需求。

聚集网络爬虫与通用网络爬虫相比，增加了链接评价模块以及内容评价模块。聚集网络爬虫爬行策略实现地关键是评价页面内容和链接地重要性，不同地方法计算出地重要性不同，由此导致链接地访问顺序不同。

* 基于内容评价地爬行策略：DeBra将文本相似度的计算方法引入到网络爬虫中，提出了 Fish Search 算法，它将用户输入的查询词作为主题，包含查询词的页面被视为与主题相关，其局限性在于无法评价页面与主题相关 度 的 高 低 。 Herseovic对 Fish Search 算 法 进 行 了 改 进 ，提 出 了 Sharksearch 算法，利用空间向量模型计算页面与主题的相关度大小

* 基于链接结构评价地爬行策略： Web 页面作为一种半结构化文档，包含很多结构信息，可用来评价链接重要性。 PageRank 算法最初用于搜索引擎信息检索中对查询结果进行排序，也可用于评价链接重要性，具体做法就是每次选择 PageRank 值较大页面中的链接来访问。 另一个利用 Web结构评价链接价值的方法是 HITS 方法，它通过计算每个已访问页面的 Authority 权重和 Hub 权重，并以此决定链接的访问顺序

* 基于增强学习地爬行策略：Rennie 和 McCallum 将增强学习引入聚焦爬虫，利用贝叶斯分类器，根据整个网页文本和链接文本对超链接进行分类，为每个链接计算出重要性，从而决定链接的访问顺序

* 基于语境图地爬行策略： Diligenti 等人提出了一种通过建立语境图（Context Graphs）学习网页之间的相关度，训练一个机器学习系统，通过该系统可计算当前页面到相关 Web 页面的距离，距离越近的页面中的链接优先访问。印度理工大学（IIT）和 IBM 研究中心的研究人员开发了一个典型的聚焦网络爬虫。 该爬虫对主题的定义既不是采用关键词也不是加权矢量，而是一组具有相同主题的网页。 它包含两个重要模块：一个是分类器，用来计算所爬行的页面与主题的相关度，确定是否与主题相关；另一个是净化器，用来识别通过较少链接连接到大量相关页面的中心页面

####  <a name="add-crawler"></a>增量式网络爬虫

增量式网络爬虫（Incremental Web Crawler）是 指 对 已 下 载 网 页 采 取 增 量式更新和只爬行新产生的或者已经发生变化网页的爬虫，它能够在一定程度上保证所爬行的页面是尽可能新的页面。 和周期性爬行和刷新页面的网络爬虫相比，增量式爬虫只会在需要的时候爬行新产生或发生更新的页面 ，并不重新下载没有发生变化的页面，可有效减少数据下载量，及时更新已爬行的网页，减小时间和空间上的耗费，但是增加了爬行算法的复杂度和实现难度。增量式网络爬虫的体系结构[包含爬行模块、排序模块、更新模块、本地页面集、待爬行 URL 集以及本地页面URL 集


增量式爬虫有两个目标：保持本地页面集中存储的页面为最新页面和提高本地页面集中页面的质量。 为实现第一个目标，增量式爬虫需要通过重新访问网页来更新本地页面集中页面内容，常用的方法有：1) 统一更新法：爬虫以相同的频率访问所有网页，不考虑网页的改变频率；2) 个体更新法：爬虫根据个体网页的改变频率来重新访问各页面；3) 基于分类的更新法：爬虫根据网页改变频率将其分为更新较快网页子集和更新较慢网页子集两类，然后以不同的频率访问这两类网页


为实现第二个目标，增量式爬虫需要对网页的重要性排序，常用的策略有：广度优先策略、PageRank 优先策略等。IBM 开发的 WebFountain是一个功能强大的增量式网络爬虫，它采用一个优化模型控制爬行过程，并没有对页面变化过程做任何统计假设，而是采用一种自适应的方法根据先前爬行周期里爬行结果和网页实际变化速度对页面更新频率进行调整。北京大学的天网增量爬行系统旨在爬行国内 Web，将网页分为变化网页和新网页两类，分别采用不同爬行策略。 为缓解对大量网页变化历史维护导致的性能瓶颈，它根据网页变化时间局部性规律，在短时期内直接爬行多次变化的网页 ，为尽快获取新网页，它利用索引型网页跟踪新出现网页


#### <a name="deep-web-crawler"></a>Deep Web 爬虫

Web 页面按存在方式可以分为表层网页（Surface Web）和深层网页（Deep Web，也称 Invisible Web Pages 或 Hidden Web）。 表层网页是指传统搜索引擎可以索引的页面，以超链接可以到达的静态网页为主构成的 Web 页面。Deep Web 是那些大部分内容不能通过静态链接获取的、隐藏在搜索表单后的，只有用户提交一些关键词才能获得的 Web 页面。例如那些用户注册后内容才可见的网页就属于 Deep Web。 2000 年 Bright Planet 指出：Deep Web 中可访问信息容量是 Surface Web 的几百倍，是互联网上最大、发展最快的新型信息资源


Deep Web 爬虫体系结构包含六个基本功能模块 （爬行控制器、解析器、表单分析器、表单处理器、响应分析器、LVS 控制器）和两个爬虫内部数据结构（URL 列表、LVS 表）。 其中 LVS（Label Value Set）表示标签/数值集合，用来表示填充表单的数据源

Deep Web 爬虫爬行过程中最重要部分就是表单填写，包含两种类型：

* 基于领域知识的表单填写：此方法一般会维持一个本体库，通过语义分析来选取合适的关键词填写表单。 Yiyao Lu等人提出一种获取 Form 表单信息的多注解方法，将数据表单按语义分配到各个组中 ，对每组从多方面注解，结合各种注解结果来预测一个最终的注解标签；郑冬冬等人利用一个预定义的领域本体知识库来识别 Deep Web 页面内容， 同时利用一些来自 Web 站点导航模式来识别自动填写表单时所需进行的路径导航


* 基于网页结构分析的表单填写： 此方法一般无领域知识或仅有有限的领域知识，将网页表单表示成 DOM 树，从中提取表单各字段值。 Desouky 等人提出一种 LEHW 方法，该方法将 HTML 网页表示为DOM 树形式，将表单区分为单属性表单和多属性表单，分别进行处理；孙彬等人提出一种基于 XQuery 的搜索系统，它能够模拟表单和特殊页面标记切换，把网页关键字切换信息描述为三元组单元，按照一定规则排除无效表单，将 Web 文档构造成 DOM 树，利用 XQuery 将文字属性映射到表单字段


###  每日一言

* 人这一辈子，怎么都是过，与其皱眉头，不如偷着乐。

<br>

转载请注明： [王龙的博客](http://www.wanglong.org.cn) >> [网络爬虫](http://www.wanglong.org.cn/2018/01/php_crawler/)